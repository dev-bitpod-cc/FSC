"""
åˆ†æ Markdown æ ¼å¼åŒ–å° Gemini File Search æŸ¥è©¢æ•ˆæœçš„å½±éŸ¿

ä½¿ç”¨æœ€æ–°çš„ Gemini File Search API æ¸¬è©¦æŸ¥è©¢æ•ˆæœ
"""

import os
import time
import json
from pathlib import Path

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("è«‹å…ˆå®‰è£: pip install google-genai")
    exit(1)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
api_key = os.getenv('GEMINI_API_KEY')

# å¦‚æœæ²’æœ‰å¾ç’°å¢ƒè®Šæ•¸å–å¾—,å˜—è©¦å¾ .env æª”æ¡ˆè®€å–
if not api_key:
    env_file = Path(__file__).parent.parent / '.env'
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if line.startswith('GEMINI_API_KEY='):
                    api_key = line.split('=', 1)[1].strip().strip('"').strip("'")

if not api_key:
    raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY")

# Store ID (Markdown ç‰ˆæœ¬)
MARKDOWN_STORE_ID = 'fscpenalties-tu709bvr1qti'

# æ¸¬è©¦æŸ¥è©¢è¨­è¨ˆ
TEST_QUERIES = [
    {
        'name': 'å…·é«”äº‹å¯¦æŸ¥è©¢',
        'query': 'è‘£äº‹å‡ºå·®å ±éŠ·ç¼ºå¤±',
        'expected': 'æ‡‰è©²æ‰¾åˆ°å…¨çƒäººå£½è‘£äº‹é•·å‰¯è‘£äº‹é•·å‡ºåœ‹è€ƒå¯Ÿçš„æ¡ˆä¾‹',
        'keywords': ['è‘£äº‹', 'å‡ºåœ‹', 'è€ƒå¯Ÿ', 'å…¨çƒäººå£½']
    },
    {
        'name': 'æ³•å¾‹ä¾æ“šæŸ¥è©¢',
        'query': 'é•åä¿éšªæ³•ç¬¬148æ¢ä¹‹3',
        'expected': 'æ‡‰è©²æ‰¾åˆ°é•åå…§éƒ¨æ§åˆ¶åˆ¶åº¦çš„æ¡ˆä¾‹',
        'keywords': ['ä¿éšªæ³•', '148', 'å…§éƒ¨æ§åˆ¶']
    },
    {
        'name': 'è™•åˆ†é‡‘é¡æŸ¥è©¢',
        'query': 'ç½°æ¬¾300è¬å…ƒçš„è£ç½°æ¡ˆä»¶',
        'expected': 'æ‡‰è©²æ‰¾åˆ°è™•åˆ†é‡‘é¡ç‚º300è¬å…ƒçš„æ¡ˆä¾‹',
        'keywords': ['300è¬', 'ç½°é°']
    },
    {
        'name': 'è¢«è™•åˆ†äººæŸ¥è©¢',
        'query': 'å…¨çƒäººå£½ä¿éšªå…¬å¸çš„é•è¦æ¡ˆä¾‹',
        'expected': 'æ‡‰è©²æ‰¾åˆ°å…¨çƒäººå£½çš„è£ç½°æ¡ˆä»¶',
        'keywords': ['å…¨çƒäººå£½']
    },
    {
        'name': 'é•è¦é¡å‹æŸ¥è©¢',
        'query': 'å…§éƒ¨æ§åˆ¶åˆ¶åº¦ç¼ºå¤±æ¡ˆä¾‹',
        'expected': 'æ‡‰è©²æ‰¾åˆ°å…§éƒ¨æ§åˆ¶ç›¸é—œçš„é•è¦',
        'keywords': ['å…§éƒ¨æ§åˆ¶', 'ç¼ºå¤±']
    },
    {
        'name': 'è¤‡åˆæŸ¥è©¢',
        'query': 'ä¿éšªæ¥­å…§éƒ¨æ§åˆ¶ç¼ºå¤±è¢«ç½°300è¬',
        'expected': 'æ‡‰è©²ç²¾ç¢ºæ‰¾åˆ°å…¨çƒäººå£½æ¡ˆä¾‹',
        'keywords': ['ä¿éšª', 'å…§éƒ¨æ§åˆ¶', '300è¬']
    },
    {
        'name': 'æ¨¡ç³ŠæŸ¥è©¢',
        'query': 'è‘£äº‹å‡ºåœ‹è€ƒå¯Ÿæ²’æœ‰äº‹å‰è¦åŠƒ',
        'expected': 'æ¸¬è©¦æ˜¯å¦èƒ½æ‰¾åˆ°ç›¸é—œæè¿°',
        'keywords': ['è‘£äº‹', 'å‡ºåœ‹', 'äº‹å‰è¦åŠƒ']
    }
]


def query_file_search_store(store_id: str, query: str, model_name: str = 'gemini-2.0-flash-exp'):
    """
    æŸ¥è©¢ Gemini File Search Store

    Args:
        store_id: File Search Store ID (ä¸å« fileSearchStores/ å‰ç¶´)
        query: æŸ¥è©¢å•é¡Œ
        model_name: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        dict: åŒ…å«å›ç­”ã€ä¾†æºæ•¸é‡ã€å¼•ç”¨ç­‰è³‡è¨Š
    """
    try:
        client = genai.Client(api_key=api_key)

        # çµ„åˆå®Œæ•´çš„ store name
        store_name = f'fileSearchStores/{store_id}'

        response = client.models.generate_content(
            model=model_name,
            contents=query,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=[store_name]
                        )
                    )
                ],
                temperature=0.1
            )
        )

        # æå–ä¾†æºæ•¸é‡å’Œå¼•ç”¨è³‡è¨Š
        sources_count = 0
        citations = []
        grounding_supports = []

        # æª¢æŸ¥ grounding metadata
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]

            # æå– grounding metadata
            if hasattr(candidate, 'grounding_metadata'):
                metadata = candidate.grounding_metadata

                # æå– grounding chunks (ä¾†æºæ–‡ä»¶)
                if hasattr(metadata, 'grounding_chunks'):
                    sources_count = len(metadata.grounding_chunks)

                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'retrieved_context'):
                            ctx = chunk.retrieved_context
                            citations.append({
                                'title': getattr(ctx, 'title', 'N/A'),
                                'uri': getattr(ctx, 'uri', 'N/A')
                            })

                # æå– grounding supports (å“ªäº›æ–‡å­—ç‰‡æ®µä¾†è‡ªå“ªå€‹ä¾†æº)
                if hasattr(metadata, 'grounding_supports'):
                    for support in metadata.grounding_supports:
                        grounding_supports.append({
                            'segment': getattr(support, 'segment', None),
                            'grounding_chunk_indices': getattr(support, 'grounding_chunk_indices', [])
                        })

        return {
            'success': True,
            'answer': response.text if response.text else '(ç„¡å›ç­”)',
            'answer_length': len(response.text) if response.text else 0,
            'sources_count': sources_count,
            'citations': citations,
            'grounding_supports_count': len(grounding_supports),
            'has_grounding': sources_count > 0
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'answer': None,
            'answer_length': 0,
            'sources_count': 0,
            'citations': [],
            'grounding_supports_count': 0,
            'has_grounding': False
        }


def analyze_answer_quality(result: dict, test: dict) -> dict:
    """
    åˆ†æå›ç­”å“è³ª

    Args:
        result: æŸ¥è©¢çµæœ
        test: æ¸¬è©¦æ¡ˆä¾‹

    Returns:
        dict: å“è³ªåˆ†æçµæœ
    """
    if not result['success'] or not result['answer']:
        return {
            'has_keywords': False,
            'keyword_count': 0,
            'is_relevant': False,
            'confidence': 'low'
        }

    answer = result['answer'].lower()
    keywords = test['keywords']

    # æª¢æŸ¥é—œéµå­—å‡ºç¾æ¬¡æ•¸
    keyword_count = sum(1 for kw in keywords if kw.lower() in answer)
    has_keywords = keyword_count > 0

    # åˆ¤æ–·ç›¸é—œæ€§
    is_relevant = keyword_count >= len(keywords) / 2

    # ä¿¡å¿ƒç¨‹åº¦
    if result['sources_count'] == 0:
        confidence = 'none'  # æ²’æœ‰ä¾†æº,å¯èƒ½æ˜¯ç·¨é€ 
    elif result['sources_count'] >= 3 and keyword_count >= 2:
        confidence = 'high'
    elif result['sources_count'] >= 1 and keyword_count >= 1:
        confidence = 'medium'
    else:
        confidence = 'low'

    return {
        'has_keywords': has_keywords,
        'keyword_count': keyword_count,
        'total_keywords': len(keywords),
        'is_relevant': is_relevant,
        'confidence': confidence
    }


def run_analysis():
    """åŸ·è¡Œåˆ†æ"""

    print("="*80)
    print("Markdown æ ¼å¼åŒ–å° File Search æŸ¥è©¢æ•ˆæœå½±éŸ¿åˆ†æ")
    print("="*80)
    print()
    print(f"Store ID: {MARKDOWN_STORE_ID}")
    print(f"æ¸¬è©¦æŸ¥è©¢æ•¸: {len(TEST_QUERIES)}")
    print()

    results = []

    for i, test in enumerate(TEST_QUERIES, 1):
        print(f"\n{'='*80}")
        print(f"æ¸¬è©¦ {i}/{len(TEST_QUERIES)}: {test['name']}")
        print(f"{'='*80}")
        print(f"æŸ¥è©¢: {test['query']}")
        print(f"é æœŸ: {test['expected']}")
        print(f"é—œéµå­—: {', '.join(test['keywords'])}")
        print()

        # åŸ·è¡ŒæŸ¥è©¢
        print("ğŸ” æŸ¥è©¢ä¸­...")
        result = query_file_search_store(MARKDOWN_STORE_ID, test['query'])

        if result['success']:
            print(f"âœ“ æŸ¥è©¢æˆåŠŸ")
            print(f"  ä¾†æºæ•¸é‡: {result['sources_count']}")
            print(f"  å›ç­”é•·åº¦: {result['answer_length']} å­—å…ƒ")
            print(f"  æœ‰å¼•ç”¨ä¾†æº: {'æ˜¯' if result['has_grounding'] else 'å¦'}")

            if result['citations']:
                print(f"  å¼•ç”¨æª”æ¡ˆ (å‰3å€‹):")
                for j, citation in enumerate(result['citations'][:3], 1):
                    print(f"    {j}. {citation['title'][:80]}")

            # åˆ†æå›ç­”å“è³ª
            quality = analyze_answer_quality(result, test)
            print(f"\n  ğŸ“Š å“è³ªåˆ†æ:")
            print(f"    é—œéµå­—åŒ¹é…: {quality['keyword_count']}/{quality['total_keywords']}")
            print(f"    ç›¸é—œæ€§: {'æ˜¯' if quality['is_relevant'] else 'å¦'}")
            print(f"    ä¿¡å¿ƒç¨‹åº¦: {quality['confidence']}")

            # é¡¯ç¤ºå›ç­”ç‰‡æ®µ
            if result['answer']:
                preview = result['answer'][:200] + '...' if len(result['answer']) > 200 else result['answer']
                print(f"\n  ğŸ’¬ å›ç­”ç‰‡æ®µ:")
                print(f"    {preview}")
        else:
            print(f"âœ— æŸ¥è©¢å¤±æ•—")
            print(f"  éŒ¯èª¤: {result['error']}")
            quality = {'confidence': 'error'}

        # å„²å­˜çµæœ
        result_record = {
            'test_name': test['name'],
            'query': test['query'],
            'expected': test['expected'],
            'keywords': test['keywords'],
            'result': result,
            'quality': quality if result['success'] else None
        }
        results.append(result_record)

        # é¿å… rate limit
        time.sleep(3)

    # å„²å­˜çµæœåˆ°æª”æ¡ˆ
    output_file = Path('data/test_results/markdown_query_analysis.json')
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n\n{'='*80}")
    print(f"åˆ†æå®Œæˆ! çµæœå·²å„²å­˜åˆ°: {output_file}")
    print(f"{'='*80}")

    # é¡¯ç¤ºæ‘˜è¦çµ±è¨ˆ
    print("\nğŸ“Š æ¸¬è©¦æ‘˜è¦:")
    print(f"  ç¸½æ¸¬è©¦æ•¸: {len(results)}")

    successful = [r for r in results if r['result']['success']]
    print(f"  æˆåŠŸæŸ¥è©¢: {len(successful)}/{len(results)}")

    if successful:
        avg_sources = sum(r['result']['sources_count'] for r in successful) / len(successful)
        print(f"  å¹³å‡ä¾†æºæ•¸: {avg_sources:.1f}")

        with_grounding = [r for r in successful if r['result']['has_grounding']]
        print(f"  æœ‰å¼•ç”¨ä¾†æº: {len(with_grounding)}/{len(successful)} ({100*len(with_grounding)/len(successful):.0f}%)")

        # å“è³ªçµ±è¨ˆ
        high_confidence = [r for r in successful if r['quality'] and r['quality']['confidence'] == 'high']
        medium_confidence = [r for r in successful if r['quality'] and r['quality']['confidence'] == 'medium']
        low_confidence = [r for r in successful if r['quality'] and r['quality']['confidence'] == 'low']
        none_confidence = [r for r in successful if r['quality'] and r['quality']['confidence'] == 'none']

        print(f"\n  ä¿¡å¿ƒç¨‹åº¦åˆ†å¸ƒ:")
        print(f"    é«˜: {len(high_confidence)} ({100*len(high_confidence)/len(successful):.0f}%)")
        print(f"    ä¸­: {len(medium_confidence)} ({100*len(medium_confidence)/len(successful):.0f}%)")
        print(f"    ä½: {len(low_confidence)} ({100*len(low_confidence)/len(successful):.0f}%)")
        print(f"    ç„¡ä¾†æº(å¯èƒ½ç·¨é€ ): {len(none_confidence)} ({100*len(none_confidence)/len(successful):.0f}%)")

    return results


if __name__ == '__main__':
    try:
        results = run_analysis()
    except KeyboardInterrupt:
        print("\n\næ¸¬è©¦ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
